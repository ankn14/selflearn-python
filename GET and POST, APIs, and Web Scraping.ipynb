{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h1 style=\"color:#D43D6E\">\nHTTP (GET and POST), APIs, and Web Scraping\n</h1>\n\n<br>\n<h2>\nThis is what Reddit says:\n</h2>\n\n<ul>\n<li>HTTP is the \"way\" in which web applications receive and send data around the world. - <i>shellsage</i></li>\n<li>\"POST\" means \"put this on the server\", and \"GET\" means \"get this from the server\". - <i>rewboss</i></li>\n<li>When you use an API, you're using an interface that lets the program you write control or access a program somebody else wrote. - <i>dmazzoni</i></li>\n<li>Usually data scraping refers to getting data out of a web page without using an API. - <i>blitzkraft</i></li>\n</ul>\n***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3 style=\"color:#D43D6E\">HTTP (GET and POST) Requests</h3>\n\nYour computer sends requests to another computer to talk with it. The request can get data or post data with respect to the other computer.\n\n**An example of a GET request**:\n- Going to <https://www.google.com> sends a GET request from your computer to Google's servers. They get you the webpage from their servers then send it to your browser.\n- Searching for things to buy at Amazon. Your computer tells Amazon to get data of the things you want to buy from their servers then send it to your browser. \n\n<br>\n\n**An example of a POST request**\n- You signing up to a website. Your login data is posted to their server then gets stored so you can login in the future.\n- You posting a picture on Instagram is just your phone sending a POST request with the picture and caption to Instagram's servers.\n\n***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3 style=\"color:#D43D6E\">APIs</h3>\n\n<h4 style=\"text-align:center\">App to API to Server to API to App</h4>\n<img src=\"https://lvivity.com/wp-content/uploads/2018/07/how-api-work.jpg\" width=\"550\">\n<br>\n<br>\n\n**An API allows my program in langauge X to talk with your program in language Y**\n- An API gives structure to GET and POST requests\n- It's a **black box**. You don't need to know how it works, you only need to know how to use it\n\n***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<h3 style=\"color:#D43D6E\">Web Scraping</h3>\n\nI want to get data from your program but you don't have an API. How do I do this?\n\n**You request the website, then get data from the HTML code**\n<br>\n\n<h4 style=\"text-align:center\">Do this with the browser's developer tools</h4>\n<img src=\"https://images.zapier.com/storage/photos/19cc8efc198be50255261d5bc1a24667.png?format=jpg\" width=\"700\">\n\n**The usual steps for this are:**\n1. Send a GET request to the website\n2. After getting the data, extract your data (usually done with the HTML tags)\n3. **Profit $$$**\n\n***\n***"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now we can show the above through code",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<strong style=\"color:#D43D6E\">Note: Do not try to memorize. Try to understand. You'll learn the syntax from Stackoverflow or Google!</strong>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# GET request\n#\n# Want to find textbooks posted on UBC Reddit?\n\nimport requests\n\nurl = 'https://api.pushshift.io/reddit/search/submission/'\nparams = {\n    'q': 'textbook',\n    'subreddit': 'UBC',\n    'size': 1\n}\n\n\nget_req = requests.get(url=url, params=params)\n\nprint(get_req.url)\n\n# Open the URL and see what's inside\n# Try doing:\n#\n#     get_req.json()\n#\n# ... To access the data with your Python program",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "https://api.pushshift.io/reddit/search/submission/?q=textbook&subreddit=UBC&size=1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# POST request\n#\n# Want to store a password on the internet that expires after 1 view?\n\nurl = 'https://file.io/'\npassword = {'text':'xke02kdsadn20alnd22skna'}\n\npost_req = requests.post(url=url, data=password)\n\n# What kind of product or service can you build with this?\n#\n# Hint: A Snapchat for files",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(post_req.json())\nprint(post_req.json().get('link'))\n\n# It says that it will expire after 14 days, but it actually expires after 1 view...\n# Try opening it!",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'success': True, 'key': '67rhqs', 'link': 'https://file.io/67rhqs', 'expiry': '14 days'}\nhttps://file.io/67rhqs\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# GET requests + web scraping\n#\n# We want to get job postings from Indeed.com\n# We are going to get python/javascript developer jobs\n# ... and they will not be for senior or intermediate level developers\n# ... also no blockchain or wordpress development\n\n# After searching on Indeed, we get this link:\n# https://ca.indeed.com/jobs?as_and=&as_phr=&as_any=python+javascript&as_not=senior+intermediate+blockchain+wordpress&as_ttl=developer&as_cmp=&jt=all&st=&as_src=&salary=&radius=25&l=Vancouver%2C+BC&fromage=7&limit=50&sort=&psf=advsrch\n# It's a long link, so let's parse it using:\n# https://www.freeformatter.com/url-parser-query-string-splitter.html\n\n# we end up with this\n\nurl = 'https://ca.indeed.com/jobs'\nparams = {\n    'as_ttl': 'developer',\n    'as_any': 'python javascript',\n    'as_not': 'senior intermediate blockchain wordpress',\n    'jt': 'all',\n    'l': 'vancouver,bc',\n    'radius': '25',\n    'limit': '50',\n    'fromage': '7'\n}\n\nreq_scrape = requests.get(url=url, params=params)\nprint(req_scrape.url)\n\n\n# Try comparing the url printed and the url posted in the comments above\n# It should exactly be the same thing\n# Now we can scrape it for job titles and company names",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "https://ca.indeed.com/jobs?as_ttl=developer&as_any=python+javascript&as_not=senior+intermediate+blockchain+wordpress&jt=all&l=vancouver,bc&radius=25&limit=50&fromage=7\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Scraping with BeautifulSoup\n#\n# The code underneath basically says \"turn the hmtl string into a parseable thing\"\n\n!pip install bs4 # we're installing the bs4 library first\n\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(markup=req_scrape.text, features='html.parser')",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use shift + tab on the find_all method to know what it does\n\njobs = soup.find_all(name='div', attrs={'data-tn-component': 'organicJob'})",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We can now find job postings data by specifying what their tags are\n# You need to go to the Indeed website to figure out what the tags are\n# Also, notice how there's a strip method in the code...\n# Indeed doesn't have a public API, so it scraping get data that's a little dirty\n# The strip method is there for some data cleaning\n\nfor job in jobs[:5]:\n    title = job.find('h2').find('a').get('title')\n    company = job.find('span', attrs={'class': 'company'}).text.strip()\n    \n    print('\\n')\n    print(title)\n    print(company)\n\n# Is this useful for you?\n# What else can you do with this?",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\n\nJunior python developer\nP3Dprinter\n\n\nJavaScript Mentor/Developer\nCampusUp Solutions Inc.\n\n\nNode.js Developer\nCoinfield\n\n\nAgile Full Stack Developer Co-Op (Summer 2019) – Analysis Workspace\nSplunk\n\n\nAgile Software Developer Co-Op (Summer 2019) – Machine Learning\nSplunk\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Afterword\n\nThis is not a technical guide on how to use the Requests and BeautifulSoup libraries. They have documentation for that. Instead, what this is is a small taste of what can be done by knowing the basics of HTTP requests, API usage, and web scraping.\n\nToo many things were skipped, but that's intentional. We believe the best way to learn stuff like this is by getting one's hand's dirty. So go build something!\n<br>\n<br>\n\n**If you want to continue learning, we encourage you to try these resources:**\n- HTTP and APIs (a 9-chapter course): <https://zapier.com/learn/apis/chapter-1-introduction-to-apis/>\n- `Requests + BeautifulSoup` tutorial: <https://www.digitalocean.com/community/tutorials/how-to-work-with-web-data-using-requests-and-beautiful-soup-with-python-3>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
